{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, image_dir, cuda=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.split('.')[-1] in ['png', 'jpg']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = cv2.imread(img_path)[:,:,::-1]\n",
    "        return image.copy(), img_path # copy to fix strides\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class TimedModule(nn.Module):\n",
    "    # utility wrapper module with timing and printing logic\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        iscuda = x.device.type == 'cuda'\n",
    "        if iscuda: # cuda execution calls are asynchronous, sync needed for proper timing\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        start_time = time.time()\n",
    "        output = self.module(x)\n",
    "        \n",
    "        if iscuda: # cuda execution calls are asynchronous, sync needed for proper timing\n",
    "            torch.cuda.synchronize()\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f'{str(self.module)}, {execution_time*1000:.4f}') # display module and time in ms\n",
    "        return output\n",
    "    \n",
    "def replace_modules(model, module_types):\n",
    "    # crawls through model, wrapping all modules of interest in TimedModule utility wrapper\n",
    "    for name, module in model.named_children():\n",
    "        # If the module is an instance of the specified types, wrap it\n",
    "        if isinstance(module, module_types):\n",
    "            setattr(model, name, TimedModule(module))\n",
    "        else:\n",
    "            # Otherwise, recursively process the current module\n",
    "            replace_modules(module, module_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling utilities\n",
    "import functools\n",
    "from torch.nn.functional import interpolate\n",
    "import sys\n",
    "\n",
    "def time_function(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        duration = (time.time()  - start_time) * 1000\n",
    "        print(f'{func.__name__}, {duration:.4f}')\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def profiled_function(func, *args, **kwargs):\n",
    "    start_time = time.time()  \n",
    "    result = func(*args, **kwargs) \n",
    "    duration = (time.time() - start_time) * 1000\n",
    "    if isinstance(func, functools.partial):\n",
    "        name = func.func.__name__\n",
    "    else:\n",
    "        name = func.__name__\n",
    "    print(f'{name}, {duration:.4f}')\n",
    "    return result\n",
    "\n",
    "# preprocessing functions\n",
    "\n",
    "def resize(img, shape):\n",
    "    return interpolate(img, shape) # could set interpolation method if performance is critical / use torch implementation\n",
    "    \n",
    "def to_01_range(img):\n",
    "    return img.type(torch.float32)/255.0\n",
    "    \n",
    "def to_nchw(img):\n",
    "    return img.permute(0,3,1,2)\n",
    "\n",
    "def to_cuda(img):\n",
    "    return img.cuda()\n",
    "\n",
    "\n",
    "# postprocessing functions\n",
    "\n",
    "\n",
    "@time_function\n",
    "def get_rescale_factor(img, inference_size):\n",
    "    x_scale_factor = 1/inference_size*img.shape[1]\n",
    "    y_scale_factor = 1/inference_size*img.shape[0]\n",
    "    return torch.tensor([x_scale_factor, y_scale_factor, x_scale_factor, y_scale_factor])\n",
    "\n",
    "@time_function\n",
    "def scale_and_convert_box(box, rescale_tensor):\n",
    "    return (box * rescale_tensor).type(torch.int32).numpy()\n",
    "\n",
    "@time_function\n",
    "def draw_box(img, box):\n",
    "    return cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), 255, 2)\n",
    "\n",
    "@time_function\n",
    "def move_results_to_cpu(results):\n",
    "    return results.cpu()\n",
    "\n",
    "# pipeline and its elements\n",
    "def postprocess(model_outs, original_image, inference_size):\n",
    "    boxes = model_outs[0][0]['boxes']\n",
    "    if boxes.device.type == 'cuda':\n",
    "        boxes = move_results_to_cpu(boxes)\n",
    "    rescale_tensor = get_rescale_factor(original_image, inference_size)\n",
    "    for box in boxes:\n",
    "        processed_box = scale_and_convert_box(box, rescale_tensor)\n",
    "        original_image = draw_box(original_image, processed_box)\n",
    "    return original_image\n",
    "        \n",
    "def timed_preprocess(img, pipe):\n",
    "    for stage in pipe:\n",
    "        img = profiled_function(stage, img)\n",
    "    return img        \n",
    "\n",
    "def run_through_pipe(img, model, preprocess_pipe, img_sizes):\n",
    "    warmup(model)\n",
    "    img_orig = img.numpy().squeeze()\n",
    "    print('Preprocess logs, ')\n",
    "    img = timed_preprocess(img, preprocess_pipe)\n",
    "    print('Model logs, ')\n",
    "    with torch.no_grad():\n",
    "        results = model(img)\n",
    "    print('Postprocess logs')\n",
    "    return postprocess(results, img_orig, img_sizes)\n",
    "\n",
    "def warmup(model):\n",
    "    device = next(model.parameters()).device\n",
    "    # Redirect stdout to suppress print statements - we don't want warmup logs\n",
    "    with SuppressPrint():\n",
    "        with torch.no_grad():\n",
    "            # warmup\n",
    "            for i in range(100):\n",
    "                model(torch.rand(1, 3, 224, 224).to(device))\n",
    "\n",
    "class SuppressPrint: # print suppressor for model warmup\n",
    "    class _Suppressor:\n",
    "        def write(self, x):\n",
    "            pass\n",
    "        \n",
    "        def flush(self):\n",
    "            pass\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = self._Suppressor()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yolo\n",
    "import pprint\n",
    "\n",
    "config = {\n",
    "'image_directory': 'data_in',\n",
    "'ckpt_path': \"yolov5s_official_2cf45318.pth\",\n",
    "'img_sizes': 672\n",
    "}\n",
    "\n",
    "# defining pre-processing pipeline\n",
    "cpu_preprocess = [to_01_range, to_nchw, functools.partial(resize, shape=(config['img_sizes'], config['img_sizes']))]\n",
    "gpu_preprocess = cpu_preprocess + [to_cuda]\n",
    "\n",
    "# creating dataset and dataloader\n",
    "dataset = ImgDataset(config['image_directory'])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False) \n",
    "# batch size set to 1 due to dataloader being bare-bones - could define collate fn or move pre-processing to dataset if\n",
    "# profiling batch-processing is required\n",
    "\n",
    "# building model to inspect it\n",
    "model = yolo.YOLOv5(80, img_sizes=config['img_sizes'], score_thresh=0.3)\n",
    "model.eval()\n",
    "\n",
    "checkpoint = torch.load(config['ckpt_path'])\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# listing unique torch modules\n",
    "modules = [type(m) for m in model.modules()] \n",
    "torchmodules = [m for m in modules if 'torch' in str(m)]\n",
    "\n",
    "pprint.pprint(list(set(torchmodules)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining which low level ops to profile, based on list above - can (should?) be changed depending on needs\n",
    "\n",
    "ops_of_interest = (\n",
    "torch.nn.modules.conv.Conv2d,\n",
    "torch.nn.modules.activation.LeakyReLU,\n",
    "torch.nn.modules.batchnorm.BatchNorm2d,\n",
    "torch.nn.modules.pooling.MaxPool2d,\n",
    "torch.nn.modules.upsampling.Upsample\n",
    ")\n",
    "\n",
    "# actually running the replacer\n",
    "replace_modules(model, ops_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#running profiler through dataset\n",
    "\n",
    "\n",
    "\n",
    "for img, img_path in dataloader:\n",
    "    plt.imshow(img.squeeze())\n",
    "    plt.title(img_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\\n### CPU PROFILING ###\\n\\n\")\n",
    "    processed = run_through_pipe(img, model.cpu(), cpu_preprocess, config['img_sizes'])\n",
    "    print(\"\\n\\n### GPU PROFILING ###\\n\\n\")\n",
    "    processed = run_through_pipe(img, model.cuda(), gpu_preprocess, config['img_sizes'])\n",
    "    \n",
    "    plt.imshow(processed)\n",
    "    plt.title(f'{img_path} processed')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 run_profiler.py > result.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
